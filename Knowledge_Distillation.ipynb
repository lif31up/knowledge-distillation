{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* [GitHub](https://github.com/lif31up/knowledge-distillation)\n",
        "* [GitBook - Knoweldge Distillation](https://lif31up.gitbook.io/blog/transfer-learning-and-knowledge-distillation/distilling-the-knowledge-in-a-neural-network)\n",
        "* [GitBook - Vision Transformer](https://lif31up.gitbook.io/blog/geometric-learning-and-computer-vision/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale)"
      ],
      "metadata": {
        "id": "qxPydHzaxMjX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8_kwdTyY0tKp"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision as tv\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEdbckX0DigU",
        "outputId": "29f15cb3-7e44-4035-ea85-35f5adb081b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at ../content/drive; to attempt to forcibly remount, call drive.mount(\"../content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('../content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEACH_SAVE_TO = \"../content/drive/MyDrive/Colab Notebooks/knoweldge_distillation/teacher.bin\"\n",
        "TEACH_LOAD_FROM = TEACH_SAVE_TO\n",
        "STNDT_SAVE_TO = \"../content/drive/MyDrive/Colab Notebooks/knoweldge_distillation/student.bin\"\n",
        "STNDT_LOAD_FROM = STNDT_SAVE_TO"
      ],
      "metadata": {
        "id": "bOsLkdS7UE_i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT/DistillViT for MNIST from scratch\n",
        "This implementation is inspired by:\n",
        "[Distilling the Knowledge in a Neural Network (2015)](https://arxiv.org/abs/1503.02531) by Geoffrey Hinton, Oriol Vinyals, Jeff Dean.\n",
        "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2021)](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n",
        "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2021)](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "Distillation (or Knowledge Distillation) is a model compression technique where a small model is trained to mimic a large, complex model by learning its \"thought process (or soft probabilities)\". Most large state-of-the-art models are incredibly accurate but come with high costs—computation, memory, and latency. Distillation captures the knowledge inside those models and packs it into a more efficient one.\n",
        "\n",
        "The Vision Transformer (ViT) attains excellent results when pretrained at sufficient scale and transferred to tasks with fewer datapoints. When pretrained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state-of-the-art image recognition benchmarks.\n",
        "\n",
        "- **Task:** Image Recognition\n",
        "- **Dataset:** MNIST-10"
      ],
      "metadata": {
        "id": "FV3vkLEHXduR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZfLSBb71Ge_"
      },
      "source": [
        "## ViTs (Vision Transformers)\n",
        "The researchers experimented with applying a standard Transformer directly to images with minimal modifications. They split an image into patches and provide the sequence of linear embeddings of these patches as input to a Transformer. Image patches are treated the same way as tokens in NLP applications. The model is trained on image classification in a supervised fashion.\n",
        "\n",
        "When trained on mid-sized datasets like ImageNet without strong regularization, these models achieve modest accuracies—a few percentage points below ResNets of comparable size. This seemingly discouraging result is expected: Transformers lack the inductive biases inherent to CNNs, such as translation equivariance and locality. As a result, they don't generalize well when trained on insufficient data.\n",
        "\n",
        "However, the picture changes when models are trained on larger datasets (14–300M images). Large-scale training trumps inductive bias. The Vision Transformer (ViT) attains excellent results when pretrained at sufficient scale and transferred to tasks with fewer datapoints. When pretrained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state-of-the-art image recognition benchmarks.\n",
        "\n",
        "### Method\n",
        "The model design follows the original Transformer as closely as possible. This intentionally simple setup offers a key advantage: scalable NLP architectures and their efficient implementations can be used almost out of the box.\n",
        "\n",
        "Figure 1 shows an overview of the model. The standard Transformer receives a 1D sequence of token embeddings as input. To handle 2D images, we reshape the image $x$ into a sequence of flattened 2D patches $x_p$ using embed $\\text{embed}(x → x_p)$:\n",
        "\n",
        "$$\n",
        "\\text{embed}(x \\in \\mathbb{R}^{H \\times W \\times C}) = x_p \\in \\mathbb{R}^{N \\times (P^2 \\cdot C)}\n",
        "$$\n",
        "\n",
        "- $C$ is the number of channels.\n",
        "- $(H, W)$ is the resolution of the original image (height and width).\n",
        "- $(P,P)$ is the resolution of each image patch (expressed as $x_p$).\n",
        "- $N = HW / P^2$ is the resulting number of patches, which also serves as the input sequence length for the Transformer.\n",
        "\n",
        "Like BERT's `[class]` token, we add a learnable embedding to the start of the embedded patch sequence $(z_0^0 x_{\\text{class}})$. Its state at the Transformer encoder's output $(z_L^0)$ becomes the image representation $y$.\n",
        "\n",
        "1. $z_0 = [ {x}_{\\text{class}}; {x}^1_p E; {x}^2_p E, ..., {x}_p^N E ] + E_{\\text{pos}} \\quad\\text{where is }E \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}, E_{\\text{pos}} \\in \\mathbb{R}^{(N + 1) \\times D}$\n",
        "2. $z’_{\\mathcal{l}} = \\text{MSA}(\\text{LN}(z_{\\mathcal{l} - 1})) + z_{\\mathcal{l - 1}}, \\quad\\text{where is }\\mathcal{l} = 1, ..., L$\n",
        "3. $z{\\mathcal{l}} = \\text{MLP}(\\text{LN}(z'_{\\mathcal{l}})) + z'_{\\mathcal{l}}, \\quad\\text{where is }\\mathcal{l} = 1, ..., L$\n",
        "4. $y = \\text{LN}(z_L^0)$\n",
        "\n",
        "Vision Transformer has much less image-specific inductive bias than CNNs. In CNNs, 2D neighborhood structure and translation equivariance are built into each layer. In ViT, only the MLPs are local and translationally equivariant, while the self-attention layers are global. Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned\n",
        "from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aWBcUsT70gld"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(ViT, self).__init__()\n",
        "    self.config = config\n",
        "    self.stacks = nn.ModuleList([EncoderStack(self.config) for _ in range(config.n_stacks)])\n",
        "    self.flatten = nn.Flatten(start_dim=1)\n",
        "    self.cls = nn.Parameter(torch.zeros(config.dim))\n",
        "    self.fc = self._get_fc(self.config.dummy).apply(self.config.init_weights)\n",
        "  # __init__\n",
        "\n",
        "  def add_cls(self, x):\n",
        "    cls = self.cls.expand(x.shape[0], 1, -1)\n",
        "    x = torch.cat([x, cls], dim=1)\n",
        "    return x\n",
        "  # add_cls\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.add_cls(x)\n",
        "    for stack in self.stacks: x = stack(x)\n",
        "    return self.fc(self.flatten(x))\n",
        "  # forward\n",
        "\n",
        "  def _get_fc(self, dummy):\n",
        "    with torch.no_grad():\n",
        "      cls = self.cls.expand(1, -1)\n",
        "      dummy = torch.cat([dummy, cls], dim=0)\n",
        "      for stack in self.stacks: dummy = stack(dummy)\n",
        "    dummy = dummy.flatten(start_dim=0)\n",
        "    return nn.Linear(dummy.shape[0], self.config.output_dim, bias=self.config.bias)\n",
        "  # _get_fc\n",
        "# Transformer\n",
        "\n",
        "class EncoderStack(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(EncoderStack, self).__init__()\n",
        "    self.config = config\n",
        "    self.mt_attn = MultiHeadAttention(config, mode=\"scaled\")\n",
        "    self.ffn = nn.ModuleList()\n",
        "    for _ in range(config.n_hidden):\n",
        "      self.ffn.append(nn.Linear(config.dim, config.dim, bias=config.bias))\n",
        "    self.activation, self.ln = nn.GELU(), nn.LayerNorm(config.dim)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    self.apply(self.config.init_weights)\n",
        "  # __init__\n",
        "\n",
        "  def forward(self, x):\n",
        "    res = x\n",
        "    x = self.ln(self.mt_attn(x) + res)\n",
        "    res = x\n",
        "    for i, layer in enumerate(self.ffn):\n",
        "      if i != len(self.ffn): x = self.dropout(self.activation(layer(x)))\n",
        "      else: x = self.dropout(layer(x))\n",
        "    return self.ln(x + res)\n",
        "  # forward\n",
        "# EncoderStack\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, config, mode=\"scaled\"):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    assert config.dim % config.n_heads == 0, \"Dimension must be divisible by number of heads\"\n",
        "    self.config = config\n",
        "    self.sqrt_d_k, self.mode = (config.dim // config.n_heads) ** 0.5, mode\n",
        "    self.w_q, self.w_k = nn.Linear(config.dim, config.dim, bias=config.bias), nn.Linear(config.dim, config.dim, bias=config.bias)\n",
        "    self.w_v, self.w_o = nn.Linear(config.dim, config.dim, bias=config.bias), nn.Linear(config.dim, config.dim, bias=config.bias)\n",
        "    self.ln, self.dropout, self.softmax = nn.LayerNorm(config.dim), nn.Dropout(config.attention_dropout), nn.Softmax(dim=1)\n",
        "\n",
        "    self.apply(self.config.init_weights)\n",
        "  # __init__\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    Q = self.w_q(x)\n",
        "    (K, V) = (self.w_k(x), self.w_v(x)) if self.mode != \"cross\" else (self.w_k(y), self.w_v(y))\n",
        "    raw_attn_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "    down_scaled_raw_attn_scores = raw_attn_scores / self.sqrt_d_k\n",
        "    if self.mode == \"masked\":\n",
        "      masked_indices = torch.rand(*down_scaled_raw_attn_scores.shape[:-1], 1) < self.config.mask_prob\n",
        "      down_scaled_raw_attn_scores[masked_indices] = float(\"-inf\")\n",
        "    attn_scores = self.softmax(down_scaled_raw_attn_scores)\n",
        "    attn_scores = self.dropout(attn_scores)\n",
        "    return self.ln(torch.matmul(attn_scores, V) + x)\n",
        "  # attn_score\n",
        "# MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedder(Dataset):\n",
        "  def __init__(self, dataset, config):\n",
        "    super(Embedder, self).__init__()\n",
        "    self.dataset, self.config = dataset, config\n",
        "    self.is_consolidated = False\n",
        "  # __init__\n",
        "\n",
        "  def __len__(self): return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    if self.is_consolidated: return self.dataset[item][0], self.dataset[item][1]\n",
        "    feature, label = self.dataset[item]\n",
        "    patches = feature.unfold(1, 30, 30).unfold(2, 30, 30).permute(1, 2, 0, 3, 4)\n",
        "    flatten_patches = torch.reshape(input=patches, shape=(9, -1))\n",
        "    label = F.one_hot(torch.tensor(label), num_classes=10).float()\n",
        "    return flatten_patches, label\n",
        "  # __getitem__\n",
        "\n",
        "  def consolidate(self):\n",
        "    buffer = list()\n",
        "    progression = tqdm(self)\n",
        "    for feature, label in progression: buffer.append((feature, label))\n",
        "    self.dataset, self.is_consolidated = buffer, True\n",
        "    return self\n",
        "  # consolidate\n",
        "# Embedder\n",
        "\n",
        "def load_CIFAR_10(transform, path='./data', trainset_len=1000, testset_len=500):\n",
        "  # trainset, testset are provided as torch.nn.utils.dataset\n",
        "  trainset = tv.datasets.MNIST(root=path, train=True, download=True, transform=transform)\n",
        "  trainset_indices = torch.randperm(trainset.__len__()).tolist()[:trainset_len]\n",
        "  trainset = Subset(dataset=trainset, indices=trainset_indices)\n",
        "  testset = tv.datasets.MNIST(root=path, train=False, download=True, transform=transform)\n",
        "  testset_indices = torch.randperm(testset.__len__()).tolist()[:testset_len]\n",
        "  testset = Subset(dataset=testset, indices=testset_indices)\n",
        "  return trainset, testset\n",
        "#load_CIFAR_10\n",
        "\n",
        "def get_transform_CIFAR_10(input_size=135):\n",
        "  return tv.transforms.Compose([\n",
        "    # 1. Augmentation for better generalization\n",
        "    tv.transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0)),\n",
        "    tv.transforms.RandomHorizontalFlip(),\n",
        "    tv.transforms.ColorJitter(brightness=0.1, contrast=0.1),  # If RGB\n",
        "    # 2. Resize and ToTensor\n",
        "    tv.transforms.Resize((input_size, input_size)),\n",
        "    tv.transforms.ToTensor(),\n",
        "    # 3. Normalization using ImageNet statistics for pre-trained models\n",
        "    tv.transforms.Normalize(\n",
        "      mean=[0.485],\n",
        "      std=[0.229]\n",
        "    ),\n",
        "  ])  # TRANSFORM\n",
        "# get_transform_CIFAR_10"
      ],
      "metadata": {
        "id": "9ST7t8myUemC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPQu3smY1RTF"
      },
      "source": [
        "### Configuration and Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "  def __init__(self, is_teacher=False):\n",
        "    self.iters = 50\n",
        "    self.batch_size = 16\n",
        "    self.trainset_len, self.testset_len = 10000, 1000\n",
        "    self.dummy = None\n",
        "\n",
        "    self.n_heads = 3\n",
        "    self.n_stacks = 6\n",
        "    self.n_hidden = 3\n",
        "    self.dim = 900\n",
        "    self.output_dim = 10\n",
        "    self.bias = True\n",
        "\n",
        "    self.dropout = 0.1\n",
        "    self.attention_dropout = 0.1\n",
        "    self.eps = 1e-3\n",
        "    self.betas = (0.9, 0.98)\n",
        "    self.epochs = 5\n",
        "    self.batch_size = 16\n",
        "    self.lr = 1e-4\n",
        "    self.alpha = 0.4\n",
        "    self.clip_grad = False\n",
        "    self.mask_prob = 0.3\n",
        "    self.init_weights = init_weights\n",
        "  # __init__\n",
        "# Config\n",
        "\n",
        "def init_weights(m):\n",
        "  if isinstance(m, nn.Linear):\n",
        "    nn.init.xavier_uniform_(m.weight)\n",
        "    if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "# init_weights"
      ],
      "metadata": {
        "id": "LNqOhd7bT9wS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()"
      ],
      "metadata": {
        "id": "5rPUUuRWaTwq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset, transform from folder\n",
        "cifar_10_transform = get_transform_CIFAR_10(input_size=90)\n",
        "trainset, testset = load_CIFAR_10(path='./data', transform=cifar_10_transform, trainset_len=config.trainset_len, testset_len=config.testset_len)"
      ],
      "metadata": {
        "id": "h_rdEWq_UvQF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embed dataset (3 times 3 patches)\n",
        "trainset = Embedder(dataset=trainset, config=config).consolidate()\n",
        "config.dummy = trainset.__getitem__(0)[0]\n",
        "trainset = DataLoader(dataset=trainset, batch_size=config.batch_size)\n",
        "testset = Embedder(dataset=testset, config=config).consolidate()\n",
        "testset = DataLoader(dataset=testset, batch_size=config.batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX5ODRRmWEPZ",
        "outputId": "a8f3674f-d5b5-4f97-f238-f47048ee3cbe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:09<00:00, 1088.39it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1187.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = ViT(config=config)"
      ],
      "metadata": {
        "id": "BAbFrvnrVAEg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m_mMQQnV0-4B"
      },
      "outputs": [],
      "source": [
        "def train(model:nn.Module, path: str, config: Config, trainset, device):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  # optim, criterion, scheduler\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=config.lr, eps=config.eps)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  scheduler = lr_scheduler.StepLR(optim, step_size=5, gamma=0.1)\n",
        "\n",
        "  progression = tqdm(range(config.iters))\n",
        "  for _ in progression:\n",
        "    for feature, label in trainset:\n",
        "      feature, label = feature.to(device, non_blocking=True), label.to(device, non_blocking=True)\n",
        "      pred = model(feature)\n",
        "      loss = criterion(pred, label)\n",
        "      optim.zero_grad()\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "    # for feature label\n",
        "    scheduler.step()\n",
        "    progression.set_postfix(loss=loss.item())\n",
        "  # for in progression\n",
        "\n",
        "  features = {\n",
        "    \"sate\": model.state_dict(),\n",
        "    \"config\": config\n",
        "  } # feature\n",
        "  torch.save(features, f\"{path}\")\n",
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBjN8BkBD9TL",
        "outputId": "d7a52256-6b82-49ce-e3dc-c5d012caa680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [10:40<00:00, 12.81s/it, loss=0.0268]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "train(model=teacher, path=TEACH_SAVE_TO, config=config, trainset=trainset, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the teacher"
      ],
      "metadata": {
        "id": "TgAdRd6DagQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataset, device):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  correct, n_total = 0, 0\n",
        "  for feature, label in tqdm(dataset):\n",
        "    feature, label = feature.to(device, non_blocking=True), label.to(device, non_blocking=True)\n",
        "    output = model.forward(feature)\n",
        "    output = torch.softmax(output, dim=-1)\n",
        "    pred = torch.argmax(input=output, dim=-1)\n",
        "    label = torch.argmax(input=label, dim=-1)\n",
        "    for p, l in zip(pred, label):\n",
        "      if p == l: correct += 1\n",
        "      n_total += 1\n",
        "  # for\n",
        "  print(f\"Accuracy: {correct / n_total:.4f}\")\n",
        "# eval"
      ],
      "metadata": {
        "id": "DIlPVKesaftv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model=teacher, dataset=testset, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgw2KuY2a7DA",
        "outputId": "9c02a794-f1da-44e6-bf7f-dc8971256227"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63/63 [00:00<00:00, 109.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BbqOeT9F-Jz"
      },
      "source": [
        "## Knowledge Distillation\n",
        "**Distillation (or Knowledge Distillation)** is a model compression technique where a small model is trained to mimic a large, complex model by learning its *\"thought process (or soft probabilities)\"*. Most large state-of-the-art models are incredibly accurate but come with high costs—computation, memory, and latency. Distillation captures the knowledge inside those models and packs it into a more efficient one.\n",
        "\n",
        "- The goal is to create a smaller, faster model that can be deployed on devices with limited computational resources.\n",
        "- A well-trained distilled model can even surpass the original, as the softened labels provide richer information and act as a regularizer.\n",
        "\n",
        "### Teacher/Student and Hard/Soft Targets\n",
        "\n",
        "Distillation relies on key concepts: soft and hard targets, and the teacher-student model relationship:\n",
        "\n",
        "- The **teacher model** is a large, pre-trained, highly accurate model. It's already an expert on the task and is typically provided as frozen.\n",
        "- The **student model** is a smaller, more efficient (with fewer layers or parameters) model.\n",
        "\n",
        "**Hard targets** are the ground-truth labels from the dataset. They provide only a single correct answer and don't convey relationships between classes—for example, that a cat is more similar to a dog than to an airplane.\n",
        "\n",
        "**Soft targets** are the output probabilities from the teacher model's final softmax layer. A well-trained teacher looking at a cat might output probabilities like `[\"airplane\": 1e-5, \"cat\": 0.90, \"dog\": 0.05]`. This reveals a small chance it could be a dog, but certainly not an airplane. This nuanced information is called *\"dark knowledge\"*, and it's what the student model learns from.\n",
        "\n",
        "### Temperature-Scaled Softmax\n",
        "\n",
        "To make the soft targets more informative, distillation uses a **temperature parameter** $T$ in the softmax function:\n",
        "\n",
        "- The standard softmax function is $P_i = \\frac{ \\exp{(z_i)} }{ \\sum_{j}{\\exp{(z_j)}} }$.\n",
        "- The temperature-scaled softmax is $P_i = \\frac{ \\exp{(z_i / T)} }{ \\sum_{j}{\\exp{(z_j / T})} }$.\n",
        "\n",
        "There are some attributes of $T$:\n",
        "\n",
        "- When $T = 1$, the function behaves as standard softmax.\n",
        "- When $T > 1$, it softens the probability distribution, making it less extreme (for example, `[0.05, 0.90, 0.05]` might become `[0.15, 0.70, 0.15]`).\n",
        "- During training, $T$ is set to a high value (like $3$ or $4$), then reset to $1$ for final inference.\n",
        "\n",
        "Softened probabilities provide much more information—they amplify the small differences between non-target classes, making it easier for the student to learn the relationships the teacher has discovered.\n",
        "\n",
        "### Understanding Total Loss\n",
        "\n",
        "Unlike typical deep learning training, the student model is trained using a combined loss function that balances two distributions:\n",
        "\n",
        "- The **distillation loss** measures how closely the student's soft predictions match the teacher's soft targets.\n",
        "- The **student loss (or hard target loss)** measures how well the student's predictions match the original ground-truth labels.\n",
        "\n",
        "This combined loss is called *“total loss”* and calculated via:\n",
        "\n",
        "$$\n",
        "\\text{Total Loss} = \\alpha \\cdot \\mathcal{L}_{\\text{distillation}} + (1 - \\alpha) \\cdot \\mathcal{L}_{\\text{student}}\n",
        "$$\n",
        "$$\n",
        "\\mathcal{L}_{\\text{distillation}}(P_{\\text{soft}} | Q_{\\text{soft}}) = \\sum_{i}{P_{\\text{soft}}(i) \\log{\\big{(} \\frac{P_{\\text{soft}}{(i)}}{Q_{ \\text{soft}}{(i)}} \\big{)}}}\n",
        "$$\n",
        "$$\n",
        "\\mathcal{L}_{\\text{student}}(P|Q_{\\text{hard}}) = \\text{CrossEntropyLoss}\n",
        "$$\n",
        "\n",
        "- $Q_{\\text{soft}}$ represents soft targets, while $Q_{\\text{hard}}$ represents hard targets.\n",
        "- $\\alpha$ is a hyperparameter that balances the two losses.\n",
        "- In this formula, $\\mathcal{L}_{\\text{distillation}}$ uses **KL Divergence**: $\\sum_{i}{P(i) \\log{\\big( \\frac{P(i)}{Q(i)} \\big{)}}}$\n",
        "\n",
        "During training, the combined loss procedure works as follows:\n",
        "\n",
        "1. Pass an input through the teacher model (which is frozen) to get $Q_{\\text{soft}}$—theses are softend using temperature param $T$.\n",
        "2. Pass the same input through the student model to get its predictions—these are also softened using $T$.\n",
        "3. Combined the two losses:\n",
        "    1. Calculate $\\mathcal{L}_{\\text{distillation}}$ between the student's and teacher's softened predictions.\n",
        "    2. Calculate $\\mathcal{L}_{\\text{student}}$ between the student's output (at $T = 1$, not softened) and the ground-truth labels (hard targets).\n",
        "4. Backpropagate the total loss and update the student's weights."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_config = copy.deepcopy(config)\n",
        "student_config.n_stacks = 3\n",
        "student_config.temperature = 2.0\n",
        "student_config.alpha = 0.35"
      ],
      "metadata": {
        "id": "Jvm3ftqkhzwO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0KLkvZmUEN8n"
      },
      "outputs": [],
      "source": [
        "def distillate(student, teacher, dataset, config, path, device):\n",
        "  student.to(device)\n",
        "  teacher.to(device)\n",
        "  student.train()\n",
        "  teacher.eval()\n",
        "\n",
        "  # optim, criterion, scheduler\n",
        "  optim = torch.optim.Adam(student.parameters(), lr=config.lr, eps=config.eps)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  scheduler = lr_scheduler.StepLR(optim, step_size=5, gamma=0.1)\n",
        "\n",
        "  progression = tqdm(range(config.iters))\n",
        "  for _ in progression:\n",
        "    for feature, label in dataset:\n",
        "      feature, label = feature.to(device), label.to(device)\n",
        "      soft_label = F.softmax(teacher(feature) / config.temperature, dim=-1)\n",
        "      output = student(feature)\n",
        "      distill_loss = criterion(output, soft_label)\n",
        "      student_loss = criterion(output, label)\n",
        "      loss = (config.alpha * distill_loss) + (1 - config.alpha) * student_loss\n",
        "      optim.zero_grad()\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "    # for feature, label\n",
        "    scheduler.step()\n",
        "    progression.set_postfix(loss=loss.item())\n",
        "\n",
        "    features = {\n",
        "      \"sate\": student.state_dict(),\n",
        "      \"config\": config\n",
        "    } # feature\n",
        "    torch.save(features, f\"{path}\")\n",
        "  # fpr\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student = ViT(student_config)"
      ],
      "metadata": {
        "id": "c9upUfjXkcq3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_data = torch.load(f=TEACH_LOAD_FROM, map_location=torch.device('cpu'), weights_only=False)\n",
        "teacher = ViT(config)\n",
        "teacher.load_state_dict(teacher_data['sate'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLZCAHlGklle",
        "outputId": "a6dbc7b3-1bd2-409a-bb47-8928d3666afb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5D5A9_IWGKrb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d545fc-a4e7-49bf-f0ba-1072188978e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [14:37<00:00, 17.55s/it, loss=0.181]\n"
          ]
        }
      ],
      "source": [
        "distillate(student=student, teacher=teacher, dataset=trainset, config=student_config, path=STNDT_SAVE_TO, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model=student, dataset=testset, device=device)"
      ],
      "metadata": {
        "id": "u1C_adBolKmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc62d5fd-cbe8-4ad2-bb08-3e93f054959d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63/63 [00:00<00:00, 260.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}